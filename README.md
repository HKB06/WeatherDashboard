# ğŸŒ¤ï¸ Kafka Weather Project

Projet de streaming mÃ©tÃ©o temps rÃ©el utilisant **Apache Kafka**, **Spark Streaming**, **HDFS** et **Flask**.

## ğŸ“‹ Description

Ce projet implÃ©mente une pipeline de donnÃ©es mÃ©tÃ©o complÃ¨te :
- **Collecte** : RÃ©cupÃ©ration des donnÃ©es mÃ©tÃ©o via l'API Open-Meteo
- **Streaming** : Transmission en temps rÃ©el via Apache Kafka
- **Traitement** : Transformations et dÃ©tection d'alertes avec Spark Streaming
- **Stockage** : Persistance dans HDFS au format Parquet
- **Visualisation** : Dashboard web avec historique et temps rÃ©el

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Open-Meteo    â”‚â”€â”€â”€â”€â–¶â”‚    Kafka    â”‚â”€â”€â”€â”€â–¶â”‚  Spark Stream   â”‚
â”‚      API        â”‚     â”‚             â”‚     â”‚  (Transform)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                     â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
                        â”‚   Frontend  â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                        â”‚   (Flask)   â”‚              â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â–¼
                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                            â”‚      HDFS       â”‚
                                            â”‚   (Parquet)     â”‚
                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ PrÃ©requis

- **Docker** et **Docker Compose**
- **Python 3.10+**
- **pip**

## ğŸš€ Installation

### 1. Cloner le projet
```bash
git clone <repo-url>
cd Sem15Kafka
```

### 2. Lancer les conteneurs Docker
```bash
docker-compose up -d
```

Services dÃ©marrÃ©s :
| Service | Port | Description |
|---------|------|-------------|
| Kafka | 9092 | Broker de messages |
| Zookeeper | 2181 | Coordination Kafka |
| Spark Master | 8080 | Interface Spark |
| Spark Worker | - | ExÃ©cution des jobs |
| HDFS Namenode | 9870 | Interface HDFS |
| HDFS Datanode | 9864 | Stockage HDFS |
| PySpark Notebook | 8888 | Jupyter + Spark |

### 3. Installer les dÃ©pendances Python (local)
```bash
cd work
pip install -r requirements.txt
```

## ğŸ¯ Lancement

### Ã‰tape 1 : CrÃ©er le topic et tester Kafka

```bash
# Terminal 1 - Producteur simple
python producer/simple_producer.py

# Terminal 2 - Consumer
python kafka_consumer.py weather_stream
```

### Ã‰tape 2 : Lancer le producteur mÃ©tÃ©o temps rÃ©el

```bash
# MÃ©tÃ©o pour Paris (toutes les 30 secondes)
python producer/current_weather_city.py --city Paris --interval 30
```

### Ã‰tape 3 : Lancer Spark Streaming (transformation + alertes)

```bash
docker exec -it pyspark_notebook spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 \
  /home/jovyan/work/spark/spark_weather_stream.py
```

### Ã‰tape 4 : Stocker dans HDFS

```bash
docker exec -it pyspark_notebook spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 \
  /home/jovyan/work/hdfs/hdfs_weather_consumer.py
```

### Ã‰tape 5 : Lancer le Dashboard

```bash
# Exporter les donnÃ©es HDFS vers JSON
docker exec -it pyspark_notebook spark-submit \
  /home/jovyan/work/hdfs/export_to_json.py

# Lancer le frontend
python frontend/app.py
```

Ouvrir **http://localhost:5000** dans le navigateur.

## ğŸ“ Structure du Projet

```
work/
â”œâ”€â”€ kafka_consumer.py              # Consumer Kafka gÃ©nÃ©rique
â”œâ”€â”€ requirements.txt               # DÃ©pendances Python
â”‚
â”œâ”€â”€ producer/
â”‚   â”œâ”€â”€ simple_producer.py         # Test Kafka basique
â”‚   â”œâ”€â”€ current_weather.py         # MÃ©tÃ©o par coordonnÃ©es
â”‚   â”œâ”€â”€ current_weather_city.py    # MÃ©tÃ©o par ville
â”‚   â””â”€â”€ weather_history_producer.py # Historique 10 ans
â”‚
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ spark_weather_stream.py    # Transformation + alertes
â”‚   â”œâ”€â”€ spark_weather_aggregates.py # AgrÃ©gats sliding windows
â”‚   â”œâ”€â”€ climate_records_detector.py # DÃ©tection records
â”‚   â”œâ”€â”€ seasonal_profile.py        # Calcul normales saisonniÃ¨res
â”‚   â”œâ”€â”€ seasonal_enricher.py       # Enrichissement temps rÃ©el
â”‚   â””â”€â”€ anomaly_detector.py        # DÃ©tection anomalies
â”‚
â”œâ”€â”€ hdfs/
â”‚   â”œâ”€â”€ hdfs_weather_consumer.py   # Stockage HDFS
â”‚   â”œâ”€â”€ weather_visualizer.py      # Visualisation batch
â”‚   â””â”€â”€ export_to_json.py          # Export HDFS â†’ JSON
â”‚
â””â”€â”€ frontend/
    â”œâ”€â”€ app.py                     # Serveur Flask
    â”œâ”€â”€ templates/index.html       # Dashboard HTML
    â””â”€â”€ weather_data.json          # Cache donnÃ©es HDFS
```

## ğŸ“Š Topics Kafka

| Topic | Description |
|-------|-------------|
| `weather_stream` | DonnÃ©es brutes Open-Meteo |
| `weather_transformed` | DonnÃ©es transformÃ©es + alertes |
| `weather_history` | DonnÃ©es historiques (10 ans) |
| `weather_enriched` | DonnÃ©es enrichies avec normales |
| `weather_anomalies` | Anomalies dÃ©tectÃ©es |

## ğŸ”” Niveaux d'Alertes

### Alertes Vent
| Niveau | Condition | Description |
|--------|-----------|-------------|
| level_0 | â‰¤ 10 m/s | Vent faible |
| level_1 | 10-20 m/s | Vent modÃ©rÃ© |
| level_2 | > 20 m/s | Vent fort |

### Alertes Chaleur
| Niveau | Condition | Description |
|--------|-----------|-------------|
| level_0 | â‰¤ 25Â°C | Normal |
| level_1 | 25-35Â°C | Chaleur modÃ©rÃ©e |
| level_2 | > 35Â°C | Canicule |

## ğŸŒ API Frontend

| Endpoint | Description |
|----------|-------------|
| `GET /` | Dashboard principal |
| `GET /api/realtime` | DonnÃ©es temps rÃ©el Kafka |
| `GET /api/hdfs` | DonnÃ©es historiques HDFS |
| `GET /api/latest` | DerniÃ¨re mesure par ville |
| `GET /api/anomalies` | Anomalies dÃ©tectÃ©es |
| `GET /api/stats` | Statistiques globales |
| `GET /api/status` | Statut Kafka/HDFS |
| `GET /api/reload` | Recharger donnÃ©es HDFS |

## ğŸ› ï¸ Commandes Utiles

```bash
# Voir les logs Kafka
docker logs kafka

# Lister les topics Kafka
docker exec kafka kafka-topics --list --bootstrap-server localhost:9092

# VÃ©rifier HDFS
docker exec namenode hdfs dfs -ls /weather/realtime

# ArrÃªter tous les conteneurs
docker-compose down

# Supprimer les volumes (reset complet)
docker-compose down -v
```

## ğŸ“ˆ Exercices ImplÃ©mentÃ©s

| # | Exercice | Fichier |
|---|----------|---------|
| 1 | Setup Kafka + producteur simple | `producer/simple_producer.py` |
| 2 | Consumer Kafka Python | `kafka_consumer.py` |
| 3 | Producteur mÃ©tÃ©o temps rÃ©el | `producer/current_weather.py` |
| 4 | Transformation Spark + alertes | `spark/spark_weather_stream.py` |
| 5 | AgrÃ©gats sliding windows | `spark/spark_weather_aggregates.py` |
| 6 | GÃ©ocodage ville/pays | `producer/current_weather_city.py` |
| 7 | Stockage HDFS partitionnÃ© | `hdfs/hdfs_weather_consumer.py` |
| 8 | Visualisation logs HDFS | `hdfs/weather_visualizer.py` |
| 9 | Historique 10 ans | `producer/weather_history_producer.py` |
| 10 | DÃ©tection records climatiques | `spark/climate_records_detector.py` |
| 11 | Profils saisonniers | `spark/seasonal_profile.py` |
| 12 | Enrichissement saisonnier | `spark/seasonal_enricher.py` |
| 13 | DÃ©tection anomalies | `spark/anomaly_detector.py` |
| 14 | Frontend Dashboard | `frontend/app.py` |

## ğŸ”— Liens Utiles

- [Apache Kafka](https://kafka.apache.org/)
- [Apache Spark](https://spark.apache.org/)
- [Open-Meteo API](https://open-meteo.com/)
- [HDFS Documentation](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html)

## ğŸ“ License

Projet Ã©ducatif - IPSSI 2026
